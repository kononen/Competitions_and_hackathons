***1. Технологический стек***

Окончательное решение (ветка подбора мероприятий) реализовано во фреймворке LangGraph, где диалог описан в виде графа состояний:

от распознавания намерения → уточнения вуза → уточнения интересов → поиска мероприятий → формирования ответа.

Параллельно реализована версия этой ветки с использованием smolagents, что позволило сравнить подходы к построению и оркестрации LLM-агентов. 

LangGraph показал более точные ответы, поэтому в итоговой версии проекта работали с ним.

***2. Используемые LLM-модели***

Для диалога с пользователем в финальной версии используется модель mistral-large-latest, построенная на архитектуре смеси экспертов, в ней 41 млрд активных параметров и 675 млрд общих. 

Она отвечает за:

\- интерпретацию пользовательского запроса;

\- формирование уточняющих вопросов;

\- генерацию финального ответа с описанием подходящих мероприятий.

Помимо LLM mistral-large-latest также рассматривали mistral-small-latest. Но она показала худшую генерацию ответов. К обоим моделям мы подключались по АПИ-ключу, не разворачивали локально (так как у нас нет таких компьютерных ресурсов)

***3. Построение векторной базы***

Для построения векторной базы мероприятий применяется мультиязычная модель эмбеддингов семейства

paraphrase-multilingual-MiniLM-L12-v2, которая обеспечивает хороший баланс между качеством и скоростью.

У нас есть векторная БД с документами (в нашем случае мероприятия в вузе) и в эмбеддинги (вектора) мы переводили запросы

***4. Метрика схожести***

Мы находили топ-к зарпосов по косинусному расстоянию, где k = до пяти (подходящих нашему запросу документов, по убыванию).

similarity = cosine\_similarity(embedding\_query, embedding\_document)

cosine\_distance(A,B)=1−A⋅B​ / (||A|| \* ||B||)

Если векторы совпадают → similarity = 1 → distance = 0

Если противоположны → similarity = –1 → distance = 2

Если ортогональны → similarity = 0 → distance = 1


***5. Релевантность и качество ответов моделей***

Качество ответа LLM в нашем кейсе на основе экспертной оценки:

1\. Mistral-large-latest:

\- Ручная экспертная оценка по критериям:

релевантность = 8/10: ответ хорошо соответствует исходному запросу,

\- Полнота = 7/10: чаще всего достаточно контекста о мероприятии для принятия решения,

\- Понятность 9/10: лаконичность и читаемость выдачи для студента).

Поведение в “пустых” сценариях 8/10:

\+ корректная обработка ситуаций, когда мероприятий по комбинации вуз + интерес не найдено;

\+ отсутствие “галлюцинаций” — модель не придумывает несуществующие события, а честно сообщает о пустой выдаче и предлагает изменить запрос.

\- В редких ситуациях при наличии у пользователя только одного подходящего мероприятия в его вузе, пользователю выводилось вместо одного раза, пять раз это одно и то же мероприятие, хотя в итоге предлагалось зарегистрироваться на одно. Данный излишний вывод не ухудшает итоговую точность ответа и предоставляемые услуги, однако перегружает внимание пользователя излишней информацией  

2\. Mistral—small-latest:

\- Ручная экспертная оценка по критериям:

релевантность = 6/10: ответ нелохо соответствует исходному запросу,

\- Полнота = 5/10: модель достаточно зависима от контекста о мероприятии для принятия решения,

\- Понятность 3/10: модель выдаёт не очень лаконичный и читаемый ответ для студента, часто дублирует одну и ту же информацию, строит иногда некорректные грамматические конструкции

Поведение в “пустых” сценариях 7/10:

\+ чаще всего корректная обработка ситуаций, когда мероприятий по комбинации вуз + интерес не найдено;

\+ отсутствие “галлюцинаций” 3/10 — модель часто придумывает несуществующие события и не предлагает скорректировать запрос.

***6. Расширяемость и дальнейшее развитие***

\- Текущая архитектура позволяет:

прозрачно заменять LLM (например, на более лёгкую или локальную модель) без изменения бизнес-логики;

\- расширять векторную базу новыми университетами и событиями без перестройки диалоговой логики;

\- дообучать или менять модель эмбеддингов для улучшения качества семантического поиска.


***\* 7. Справка: архитектура Mixture of Experts (MoE)***

Mixture of Experts (MoE) — архитектура больших языковых моделей (LLM), при которой вычислительная нагрузка на определённом слое или операции распределяется между несколькими «экспертными» подсетями. 

llmstudio.ru

gerwin.io

Эксперты — небольшие, независимые нейронные сети, обычно имеющие схожую архитектуру (например, feed-forward сети), но разные параметры. Каждый эксперт в процессе обучения неявно специализируется на обработке определённых типов данных, паттернов или концепций.

Разработчики потратят много времени и денег, чтобы её обучить, в ней много весов, но при предсказании (инференсе) она быстрая и задействована малая часть экспертов (меньше существенно весов). Эксперты подбираются в зависимости от задачи пользователя


